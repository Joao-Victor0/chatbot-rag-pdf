import os

from extract_pdf_to_json import PdfToJson
from langchain.prompts import ChatPromptTemplate

from langchain_chroma import Chroma
from langchain_community.document_loaders import JSONLoader

from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain.chains import create_retrieval_chain

from langchain_ollama import ChatOllama, OllamaEmbeddings

class AgentWithKnowledge:
    def __init__(self):
        self.model = ChatOllama(model="deepseek-r1:latest")
        self.retriever = None
        self.retriever_chain = None

        #print("\n[] [DEBUG] Classe AgentWithKnowledge est√° sendo instanciada (s√≥ deve acontecer junto com o cache).")

    def setup_knowledge_base(self, pdf_path: str):
        embedding_function = OllamaEmbeddings(model="llama3")

        json_output_path="output/extracted_content.json"
        chroma_db_path="./chroma_db"

        #Vector Base, Embeddings and Retriever
        if os.path.exists(chroma_db_path):
            #print("‚ö°Ô∏è [DEBUG] CAMINHO R√ÅPIDO: Encontrou `chroma_db` e est√° carregando do disco.")
            db = Chroma(persist_directory=chroma_db_path, embedding_function=embedding_function)

        else:
            #PDF To JSON
            #print("üê¢ [DEBUG] CAMINHO LENTO: N√£o encontrou `chroma_db`. Criando um novo banco de dados (ETAPA LENTA).")
            os.makedirs(os.path.dirname(json_output_path), exist_ok=True)
            if not os.path.exists(json_output_path):
                PdfToJson.extract_pdf_content(pdf_path=pdf_path)

            #Loading Documents
            loader = JSONLoader(file_path=json_output_path, jq_schema='.', text_content=False)
            documents = loader.load()

            #Create and Persist Vector Base
            db = Chroma.from_documents(
                documents=documents,
                embedding=embedding_function,
                persist_directory=chroma_db_path
            )

        self.retriever = db.as_retriever()
        #print("‚úÖ [DEBUG] Retriever foi configurado.")

        #Context and Question
        system_template = """Voc√™ √© um assistente de IA especialista em analisar documentos e responder perguntas acerca
        do documento. 
        Se a informa√ß√£o n√£o estiver no contexto, diga 'N√£o encontrei a resposta no documento'
        Responda as perguntas estritamente baseado no contexto fornecido pelo documento abaixo:

        Contexto: {context}
        """

        human_template = "Quest√£o: {input}"

        #Prompt and Model
        prompt = ChatPromptTemplate.from_messages([
            ("system", system_template), 
            ("human", human_template)
        ])

        #Document and Retrieval Chains
        document_chain = create_stuff_documents_chain(self.model, prompt)
        self.retrieval_chain = create_retrieval_chain(self.retriever, document_chain)


    def ask(self, query:str):
        #Response
        #print(f"üí¨ [DEBUG] M√©todo `ask` chamado com a query: '{query}'")
        if not self.retrieval_chain:
            return "Erro: A base de conhecimento n√£o foi configurada. Chame o m√©todo setup_knowledge_base primeiro."
        
        response = self.retrieval_chain.invoke({"input": query})
        return response.get('answer', "N√£o foi poss√≠vel encontrar uma resposta.")